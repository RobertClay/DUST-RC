#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
This file is used for interpreting the results of the multiple repeat experiments
of the unscented kalman filter over stationsim generated by `arc.py`. 

We take a matrix
of distances between truth and some estimate over time and agents. We take the median
of each column as the median agent error and then a further median of column medians
as the grand median error for an experiment.

There are two types of plots that correspond to the data produced by ukf 
experiments. 

First we have numpy plots. These take data in the form of .npy arrays where 
the grand medians of our estimates have already been calculated. We parse
some large number of 3x1 numpynarrays into both choropleth style plots giving
simply the best performing of 3 estimates (lowest grand median error) 
and the exact error trajectories of 3 estimates to compare rather than just the minima.
The latter can be useful if two estimates perform similarly well and only the 
minimum doesnt provide sufficient information.

The latter of these plots also comes in a 3d version for nicer visualisation.
These trajectories can be exponential and as such we use the twice logged trajectories
instead for clarity.

The second types of plots assume grand medians for each experiment have not been 
calculated. We parse entire ukf classes, calculating the 3x1 numpy arrays post-hoc
before doing similar chloropleth style plots only. These plots are usually
only for assessing one estimate and as such we only produce chloropleths here.

However, we instead also produce boxplots, assessing the grand median errors 
for experiments varying over two parameters (such as population and proportion 
observed).


NOTE: Import data from `arc.py` with the following in a bash terminal:
    
scp remote_file_source* local_file_destination.

Given the appropriate directories e.g.

scp medrclaa@arc3.leeds.ac.uk:/nobackup/medrclaa/dust/Projects/ABM_DA/experiments/ukf_experiments/ukf_results/agg* /home/rob/dust/Projects/ABM_DA/experiments/ukf_experiments/ukf_results/.

or the proxy jump version via remote access

scp -oProxyJump=user@remote-access.leeds.ac.uk
e.g.
scp -oProxyJump=medrclaa@remote-access.leeds.ac.uk medrclaa@arc4.leeds.ac.uk:/nobackup/medrclaa/dust/Projects/ABM_DA/experiments/ukf_experiments/results/agg* /Users/medrclaa/new_aggregate_results


NOTE: A depickling error may occur with scipy.CKDTree. If it does roll back to scipy 1.2.
conda install scipy=1.2
"""


import os
import sys

import pandas as pd
import glob
import numpy as np
import copy

import seaborn as sns
import matplotlib.lines as lines
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.colors as colors
import matplotlib.patheffects as pe
import matplotlib.cm as cm
from mpl_toolkits.axes_grid1 import make_axes_locatable
import matplotlib.pyplot as plt


sys.path.append("../../..")
sys.path.append("../..")
sys.path.append("..")
#sys.path.append("../ukf_old")

from stationsim.ukf2 import pickle_main, truth_parser, preds_parser

from modules.ukf_plots import L2s as L2_parser
from modules.ukf_fx import HiddenPrints

"""
path appends to keep pickle happy when loading in older experiments.
"""



"import ukf and transition function fx"

"import numpy and various plotting tools"


# %%


class grand_plots:

    """class for results of multiple ukf runs

    """

    def __init__(self, params, save, restrict=None, **kwargs):
        """initialise plot class

        Parameters
        ------
        params : dict
            `params` dictionary defines 2 parameters to extract data over,
            file source, and plot save destination.

            e.g.
            depickle_params = {
                    "agents" :  [10,20,30],
                    "bin" : [5,10,25,50],
                    "source" : "/media/rob/ROB1/ukf_results/ukf_",
                    "destination" : "../plots/"
            }
            Searches for files with 10,20, and 30 "agents" (population size) 
            and 5, 10, 25, and 50 experiment 2 square grid size "bin".
            It extracts files from the source and produces plots in destination


        save : bool
            `save` plots?

        restrict : func
            `restrict` function allows you to split up the distances data you
            plot. For example, in experiment 1 we have an observed/unobserved data
            split. This function allows you to restrict the plotting to observed
            or unobserved plotting only so we can properly observe the split. 

            The aim is to get this working with obs_array for new pickles 
            such that one can plot unobserved/ aggregate/observed depickle plots
            all separately.

        **kwargs : **kwargs
            `kwargs` keywords arguements for restrict function. For example,
            ex1_restrict is the restrict function for experiment 1. It needs a 
            boolean determining whether to plot observed/unobserved. This is 
            just a generalised version so we can hopefully work with both this
            current version for old pickles and the obs_array stuff for new pickles.
        """
        self.param_keys = [key for key in params.keys()]
        self.p1 = params[self.param_keys[0]]
        self.p2 = params[self.param_keys[1]]
        self.source = params["source"]
        self.destination = params["destination"]
        self.save = save
        self.restrict = restrict
        self.kwargs = kwargs

    "various data parsers"
    
    def numpy_parser(self):
        """ extract numpy form results for plotting (see experiment 0)

        - loop over two param_keys provided.
        - for each pair of keys extract files.
        - return a dictionary with 3x1 numpy arrays for each pair of parameters.

        Returns
        ------

        errors : dict
            dictionary with p1/p2 parameter pair keys. Each key has a list of 
            3x1 numpy arrays comparing L2 distances between pseudo-truths and
            noisy observations, stationsim forecasts, and ukf assimilations.        
        """
        #loop over keys
        keys = self.param_keys
        #init main dictionary
        errors = {}
        for i in self.p1:
            #init sub dictiory for p1
            errors[i] = {}
            for j in self.p2:
                
                #glob files from source with given p1 and p2
                f_name = self.source + f"{keys[0]}_{i}_{keys[1]}_{j}*"
                files = glob.glob(f_name)
                #init sub-sub list for files with given p1 and p2
                errors[i][j] = []
                for file in files:
                    #append subsub list with numpy files according to names.
                    errors[i][j].append(np.load(file))

        return errors
          
    def depickle_data_parser(self, instance):
        """extracts truths and preds from a ukf instance (ex1/3)
        
        Parameters
        --------
        instance : cls
            ukf `instance` to extract data from
        Returns
        ------

        truth : array_like
            `a` noisy observations of agents positions
        preds : array_like
            `b` ukf predictions of said agent positions
        """

        """pull actual data. note a and b dont have gaps every sample_rate
        measurements. Need to fill in based on truths (d).
        """
        truth = np.vstack(instance.truths)
        preds2 = np.vstack(instance.ukf_histories)

        "full 'd' size placeholders"
        preds = np.zeros((truth.shape[0], instance.pop_total*2))*np.nan

        "fill in every sample_rate rows with ukf estimates and observation type key"
        "!!theres probably an easier way to do this"
        for j in range(int(preds.shape[0]//instance.sample_rate)):
            preds[j*instance.sample_rate, :] = preds2[j, :]

        nan_array = np.ones(shape=truth.shape,)*np.nan
        for i, agent in enumerate(instance.base_model.agents):
            array = np.array(agent.history_locations)
            index = np.where(array != None)[0]
            nan_array[index, 2*i:(2*i)+2] = 1

        return truth*nan_array, preds*nan_array

    def data_extractor(self):
        """main function for pulling position data from pickles for analysis (ex1/3)

        This is function looks awful... because it is. 
        Heres what it does:

        - build grand dictionary L2
        - loop over first parameter e.g. population size
            - create sub dictionary for given i L2[i]
            - loop over second parameter e.g. proportion observed (prop)
                - create placeholder list sub_L2 to store data for given i and j.
                - load each ukf pickle with the given i and j.
                - for each pickle extract the data, calculate L2s, and put the 
                    grand median L2 into sub_L2.
                - put list sub_L2 as a bnpy array into 
                dictionary L2[i] with key j.

        This will output a dictionary where for every pair of keys i and j , we accquire
        an array of grand medians.

        Returns
        ------
        L2 : dict
             dictionary of `L2` distances between ground truth and ukf predictions 
             over 2 parameters. We have keys [i][j] corresponding to the ith 
             value of parameter 1 (e.g population) and jth value of parameter 2
             (e.g proportion observed). Each pair of keys will contain a list of 
             numpy arrays. Each array is a scalar grand median of an L2 distance matrix
             output by ukf_plots.L2s

        """
        "names of first and second parameters. e.g. agents and prop"
        keys = self.param_keys
        "placeholder dictionary for all parameters"
        L2 = {}
        "loop over first parameter. usually agents."
        for i in self.p1:
            print(i)
            "sub dictionary for parameter i"
            L2[i] = {}
            for j in self.p2:
                print(i, j)
                "file names for glob to find. note wildcard * is needed"
                f_name = self.source + f"*{keys[0]}_*{i}_{keys[1]}_*{j}-*"
                "find all files with given i and j"
                files = glob.glob(f_name)
                "placeholder list for grand medians of UKF runs with parameters i and j"
                sub_L2 = []
                for file in files:
                    "open pickle"
                    f = open(file, "rb")
                    u = pickle_main(os.path.split(
                        file)[1], os.path.split(file)[0]+"/", True)
                    f.close()
                    "pull raw data"
                    try:
                        truth = truth_parser(u)
                        preds = preds_parser(u, True)
                    except:
                        truth, preds = self.depickle_data_parser(u)
                    # find L2 distances
                    distances = L2_parser(truth[::u.sample_rate, :],
                                          preds[::u.sample_rate, :])
                    if self.restrict is not None:
                        # if taking some sub set of distances do it now
                        # e.g. observed/unobserved
                        distances = self.restrict(distances, u, self.kwargs)

                    "add grand median to sub_L2"
                    sub_L2.append(np.nanmedian(
                        np.nanmedian(distances, axis=0)))
                    "stack list of grand medians as an nx1 vector array"
                    "put array into grand dictionary with keys i and j"
                L2[i][j] = np.hstack(sub_L2)

        return L2

    def gates_extractor(self):
        """ function for extracting gate choices from pickles (ex3)
        

        Returns
        -------
        gate_distances : dict
            `gate_distances` dictionary of arrays of error gate metrics
            over time for provided set of pickles.
        """
        keys = self.param_keys
        gate_distances = {}
        # loop over first parameter. usually population.
        for i in self.p1:
            print(i)
            # sub dictionary for parameter i
            gate_distances[i] = {}
            for j in self.p2:
                print(i, j)
                # file names for glob to find. note wildcard * is needed
                f_name = self.source + f"*{keys[0]}_*{i}_{keys[1]}_*{j}-*"
                # find all files with given i and j
                files = glob.glob(f_name)
                # placeholder list for grand medians of UKF runs with parameters i and j
                sub_distances = []
                for file in files:
                    # open pickle
                    f = open(file, "rb")
                    u = pickle_main(os.path.split(file)[1], os.path.split(file)[0]+"/", True)
                    f.close()
                    # pull raw data
                    self.jump_rate = u.jump_rate
                    true_gates = u.true_gate
                    predicted_gates = np.vstack(u.estimated_gates)
                    # find L2 distances
                    distances = (true_gates - predicted_gates != 0) * 1
                    
                    sub_distances.append(np.sum(distances, axis=1))
                    # stack list of grand medians as an nx1 vector array
                    # put array into grand dictionary with keys i and j
                gate_distances[i][j] = sub_distances
                
        return gate_distances
                
    """dictionaries to pandas data frames"""
    
    def numpy_extractor(self, L2s):
        """convert dictionary of numpy data from numpy_parser into pandas table (ex0)
        
        This aggregates a dictionary of data into a pandas data frame.
        Data for each pair of parameter keys (e.g. population and proportion 
        observed.) are sorted and aggregated into rows. These rows are then
        used by chloropleths and trajectory plots later.

        -loop over two param_keys
        - for every file for each pair of keys convert each 
            3x1 numpy array into a pandas row
        - p1 | p2 | obs | forecasts | ukf |
        - parameter 1| parameter2| pure observation estimate| prediction estimate| 
        ukf estimate
        - aggregate for each parameter pair using medians to get overall
        results for each parameter pair
        - record which of the three estimates performs best (minimum grand median
        error) and 

        Params
        ------

        L2s : dict
            dictionary with p1/p2 parameter pair keys. Each key has a list of 
            3x1 numpy arrays comparing L2 distances between pseudo-truths and
            noisy observations, stationsim forecasts, and ukf assimilations.

        Returns
        ------
        frame, best_array : array_like

            `frame` provides complete pandas data frame for all experiments
            with columns p1 | p2 | obs | forecasts | ukf | best

            `best_array` provides an intiger 0, 1, or 2 for the best performing
            estimator obs, forecasts, or ukf. Each row indicates the ith values of 
            p1 and each column represents the jth value of p2.

        """
        #build placeholder data frame 
        keys = self.param_keys
        columns = [keys[0], keys[1], "obs", "forecasts", "ukf"]
        frame = pd.DataFrame(columns=columns)
        
        #loop over dictionary and load numpy arrays into pandas rows in frame
        for i in self.p1:
            for j in self.p2:
                data = L2s[i][j]
                for item in data:
                    new_row = pd.DataFrame(
                        [[float(i), float(j)]+list(item)], columns=columns)
                    frame = pd.concat([frame, new_row])

        #aggregate by rate then noise using median
        frame = frame.groupby(by=["rate", "noise"]).median()
        #which estimate had lowest grand median error
        best = frame.idxmin(axis=1)
        #indicator for which estimate is best. useful for plotting later.
        best.loc[best == "obs"] = 0
        best.loc[best == "forecasts"] = 1
        best.loc[best == "ukf"] = 2
        # add best performing estimate column
        frame["best"] = best

        best_array = np.zeros(shape=(len(self.p1), len(self.p2)))
        for i, x in enumerate(self.p1):
            for j, y in enumerate(self.p2):
                best_array[i, j] = int(frame.loc[x].loc[y]["best"])

        return frame, best_array

    def gates_data_frame(self, gate_distances):
        """stack dictionaries of gate errors into a pandas data frame.
        

        Parameters
        ----------
        gate_distances : dict
            see `gates_extractor`

        Returns
        -------
        data_frame : array_like
            pandas data frame stacking dictionary into 4 columns. 
            have time, parameter 1, parameter 2, and error metric.

        """
        data_frame = pd.DataFrame()
        keys = self.param_keys
        for i in self.p1:
            for j in self.p2:
                gates_list = gate_distances[i][j]
                for data in gates_list:
                    sub_frame = pd.DataFrame(data)
                    sub_frame.columns = ["Error"]
                    sub_frame["time_points"] = sub_frame.index * j
                    sub_frame[keys[0]] = str(i).zfill(2)
                    sub_frame[keys[1]] = str(j).zfill(2)
                    data_frame = pd.concat([data_frame, sub_frame])
                
        return data_frame
        
    def data_framer(self, L2):
        """ turns dictionary of L2 position arrays into pandas dataframe for easier plotting (ex1/3)

        Returns
        ------
        error_frame : array_like

            `error_frame` pandas data frame where each row has parameters 1 and 2 
            as well as a grand median value for each pair. Each parameter pair will
            have a sample of grand median L2s which used as a boxplot sample or
            further aggregated for choropleths.

        """
        sub_frames = []
        keys = self.param_keys
        for i in self.p1:
            for j in self.p2:
                "extract L2s from L2 dictionary with corresponding i and j."
                L2s = L2[i][j]
                sub_frames.append(pd.DataFrame(
                    [[i]*len(L2s), [j]*len(L2s), L2s]).T)

        "stack into grand frames and label columns"
        error_frame = pd.concat(sub_frames)
        error_frame.columns = [keys[0], keys[1], "Grand Median L2s"]

        return error_frame

    def choropleth_array(self, error_frame):
        """converts pandas frame into generalised numpy array for choropleth (ex1/3)

        Returns
        ------

        error_array : array_like
            `error_array` numpy array whose ith row and jth column correspond
            to the ith and jth items of the parameter keys. The i,jth entry of
            the array gives the overall grand median agent L2error for choropleths
        """
        error_frame2 = error_frame.groupby(by=[str(self.param_keys[0]),
                                               str(self.param_keys[1])]).median()
        error_array = np.ones((len(self.p1), len(self.p2)))*np.nan

        for i, x in enumerate(self.p1):
            for j, y in enumerate(self.p2):
                error_array[i, j] = error_frame2.loc[(x, y), ][0]

        return error_array

    """
    plots
    """

    def comparison_choropleth(self, n, L2, best_array, xlabel, ylabel, title):
        """plot choropleth style for which is best our obs forecasts and ukf

        Parameters
        ------
        L2 , best_array : array_like
            `L2` and `best_array` defined above

        n : int
            `n` population size

        xlabel, ylabel, title : str

            x axis/ y axis labels `xlabel` `ylabel` and plot `title`

        save : bool
            `save` plot?
        """

        f, ax = plt.subplots(figsize=(12, 8))
        "cbar axis"
        divider = make_axes_locatable(ax)
        cax = divider.append_axes("right", size="5%", pad=0.05)
        colours = ["yellow", "orangered", "skyblue"]
        "custom discrete 3 colour map"
        cmap = colors.ListedColormap(colours)
        cmaplist = [cmap(i) for i in range(cmap.N)]
        cmap = colors.LinearSegmentedColormap.from_list(
            "custom_map", cmaplist, cmap.N)
        bounds = [0, 1, 2, 3]
        norm = colors.BoundaryNorm(bounds, cmap.N)

        "imshow plot and colourbar"
        im = ax.imshow(best_array, origin="lower", cmap=cmap, norm=norm)

        for i, x in enumerate(self.p1):
            for j, y in enumerate(self.p2):
                best = L2.loc[x].loc[y][["obs", "forecasts", "ukf"]].min()
                best = round(best, 2)
                ax.annotate(s=best, xy=(np.arange(-0.25, len(self.p2), 1)[j],
                                        np.arange(0, len(self.p1), 1)[i]), color="k")

        #"""alternative continous contour plot idea for more "spatially real" mapping"""
        #grid = np.meshgrid(noises,rates)
        #im = plt.contourf(grid[0],grid[1],best_array,cmap=cmap,levels=[0,1,2,3])
        plt.ylim([0, 2])
        cbar = plt.colorbar(im, cax=cax, ticks=np.arange(
            0, len(bounds)-1, 1)+0.5, boundaries=[0, 1, 2, 3])
        cbar.set_label("Minimum Grand Median L2 Error")
        cbar.set_alpha(1)
        cbar.draw_all()

        "labelling"
        cbar.ax.set_yticklabels(("Observations", "StationSim", "UKF Assimilations"),
                                rotation=30, size=12, rotation_mode="anchor")
        ax.set_xticks(np.arange(len(self.p2)))
        ax.set_yticks(np.arange(len(self.p1)))
        ax.set_xticklabels(self.p2)
        ax.set_yticklabels(self.p1)
        ax.set_xticks(np.arange(-.5, len(self.p2), 1), minor=True)
        ax.set_yticks(np.arange(-.5, len(self.p1), 1), minor=True)
        ax.grid(which="minor", color="k", linestyle="-", linewidth=2)
        ax.set_xlabel(xlabel)
        ax.set_ylabel(ylabel)
        ax.set_title(title)
        if self.save:
            plt.tight_layout()
            plt.savefig(self.destination + f"{n}_base_config_test.pdf")

    def comparisons_3d(self, n, data, best_array):
        """3d version of plots 2 based on Minh's code

        Parameters
        ------
        data2,best_array : array_like
            `data2` and `best_array` defined above

        n,rates,noises: list
            `n` `rates` `list` lists of each parameter defined above
        save : bool
            `save` plot?
        """
        # first make list of plots
        colours = ["yellow", "orangered", "skyblue"]

        "init and some labelling"
        fig = plt.figure(figsize=(12, 12))
        ax = fig.add_subplot(111, projection='3d')
        ax.set_xlabel('Observation Noise', labelpad=20)
        ax.set_ylabel("Data Assimilation Window", labelpad=20)
        ax.set_zlabel(
            'Log(Log(x+1)+1) Grand Median L2 Error (30 Agents)', labelpad=20)
        ax.view_init(30, 225)

        "take each rate plot l2 error over each noise for preds obs and ukf"
        for i, p1 in enumerate(self.p1):

            def logx2(x):
                # take double log(log(x+1)+1)
                return np.log1p(np.log1p(x))
            
            # split data by parameter one (usually population.).
            # take double logs of data too so the plots are clearer.
            sub_data = data.loc[p1]
            preds = list(logx2(sub_data["forecasts"]))
            ukf = list(logx2(sub_data["ukf"]))
            obs = list(logx2(sub_data["obs"]))

            #
            xs = np.arange(len(self.p2))
            ys = [i]*len(self.p2)
            ax.plot(xs=xs, ys=ys, zs=obs, color=colours[0], linewidth=4,
                    path_effects=[pe.Stroke(linewidth=6, foreground='k', alpha=1),
                                  pe.Normal()], alpha=0.8)

            ax.plot(xs=xs, ys=ys, zs=preds, color=colours[1], linewidth=4,
                    linestyle="-.", path_effects=[pe.Stroke(linewidth=6, foreground='k', alpha=1),
                                                  pe.Normal()], alpha=0.6)
            ax.plot(xs=xs, ys=ys, zs=ukf, color=colours[2], linewidth=4,
                    linestyle="--", path_effects=[pe.Stroke(offset=(2, 0), linewidth=6,
                                                            foreground='k', alpha=1),
                                                  pe.Normal()], alpha=1)

        "placeholder dummies for legend"
        s1 = lines.Line2D([-1], [-1],
                          color=colours[0],
                          label="Observed",
                          linewidth=4,
                          linestyle="-",
                          path_effects=[pe.Stroke(linewidth=6, 
                                                  foreground='k', alpha=1), pe.Normal()])
        s2 = lines.Line2D([-1], [-1], color=colours[1],
                          label="StationSim",
                          linewidth=4,
                          linestyle="-.",
                          path_effects=[pe.Stroke(linewidth=6, 
                                                  foreground='k', alpha=1), pe.Normal()])
        s3 = lines.Line2D([-1], [-1],
                          color=colours[2],
                          label="UKF Assimilations", linewidth=4, linestyle="--",
                          path_effects=[pe.Stroke(offset=(2, 0), linewidth=6, 
                                                  foreground='k', alpha=1), pe.Normal()])

        "rest of labelling"
        ax.set_xticks(np.arange(0, len(self.p2)))
        ax.set_xticklabels(self.p2)
        ax.set_yticks(np.arange(0, len(self.p1)))
        ax.set_yticklabels(self.p1)
        ax.legend([s1, s2, s3], ["Observed", "StationSim", "UKF Assimilations"])
        plt.tight_layout()
        "save?"
        if self.save:
            plt.tight_layout
            plt.savefig(self.destination + f"3d_{n}_error_trajectories.pdf")
            plt.close()
  
    def choropleth_plot(self, error_array, xlabel, ylabel, title):
        """choropleth style plot for grand medians

        Parameters
        ------
        data : array_like
            L2 `data` matrix from grand_L2_matrix
        n,bin_size : float
            population `n` and square size `bin_size`
        save : bool
            `save` plot?

        """
        "rotate  so population on x axis"
        data = np.rot90(error_array, k=1)
        "flip so proportion goes upwards so imshow `origin=lower` is true"
        data = np.flip(data, axis=0)
        "put nan values to white"
        data2 = np.ma.masked_where(np.isnan(data), data)

        "initiate plot"
        f, ax = plt.subplots(figsize=(8, 8))
        "colourmap"
        cmap = copy.copy(cm.get_cmap("viridis"))
        "set nan values for 100% unobserved to white (not black because black text)"
        cmap.set_bad("white")

        im = ax.imshow(data2, interpolation="nearest",
                       cmap=cmap, origin="lower")

        "text on top of squares for clarity"
        for i in range(data.shape[0]):
            for j in range(data.shape[1]):
                plt.text(j, i, str(data[i, j].round(2)), ha="center", va="center", color="w",
                         path_effects=[pe.Stroke(linewidth=0.7, foreground='k')])

        "colourbar alignment and labelling"
        divider = make_axes_locatable(ax)
        cax = divider.append_axes("right", size="5%", pad=0.05)
        cbar = plt.colorbar(im, cax, cax)

        "labelling"
        ax.set_xticks(np.arange(len(self.p1)))
        ax.set_yticks(np.arange(len(self.p2)))
        ax.set_xticklabels(self.p1)
        ax.set_yticklabels(self.p2)
        ax.set_xticks(np.arange(-.5, len(self.p1), 1), minor=True)
        ax.set_yticks(np.arange(-.5, len(self.p2), 1), minor=True)
        ax.grid(which="minor", color="k", linestyle="-", linewidth=2)
        ax.set_xlabel(xlabel)
        ax.set_ylabel(ylabel)
        plt.title = title + " Choropleth"
        cbar.set_label(title + " Grand Median L2s")

        "save"
        if self.save:
            plt.savefig(self.destination + title + "_Choropleth.pdf")

    def boxplot(self, error_frame, xlabel, ylabel, title):
        """produces grand median boxplot for all 30 ABM runs for choropleth plot


        Parameters
        ------
        frame : array_like
            L2 `data` matrix from grand_L2_matrix
        n,bin_size : float
            population `n` and square size `bin_size`
        save,seperate : bool
            `save` plot?
            `seperate` box plots by population or have one big catplot?

        """
        keys = self.param_keys
        data = error_frame
        f_name = title + "_boxplot.pdf"
        y_name = "Log Grand Median L2s (log(metres))"

        error_frame[y_name] =  np.log(error_frame["Grand Median L2s"])
        f, ax = plt.subplots(1, 1)
        cat = sns.catplot(x=str(keys[1]), y=y_name, col=str(
            keys[0]), kind="box", data=data, ax =ax)
        
        plt.tight_layout()
        #for i, ax in enumerate(cat.axes.flatten()):
        #    ax.set_xlabel(xlabel)
        ax.set_ylabel(ylabel)
        #    ax.set_title(str(keys[0]).capitalize() + " = " + str(self.p1[i]))
        plt.title = title
        if self.save:
            plt.tight_layout()
            plt.savefig(self.destination + f_name)
            
    def gates_data_lineplot(self, data_frame, xlabel, ylabel, title):
        """ plot confidence interval for rjukf gate convergance
        

        Parameters
        ----------
        data_frame : array_like
            pandas `data_frame` containing gate distance errors for multiple
            rjukf runs to be aggregated by lineplot
            DESCRIPTION.
        xlabel, ylabel, title : str
            `xlabel` x label `ylabel` y label and `title` custom definitions
            for custom labels.

        Returns
        -------
        None.

        """
        # only do legend on first pass
        legend = True
        for i in self.p1:
            data = data_frame.loc[data_frame[self.param_keys[0]].isin([str(i)])]
            key = self.param_keys[1]
            f, ax = plt.subplots(1,1)
            g = sns.lineplot(x = "time_points", y = "Error", data = data,
                              hue = key, style = key,
                              palette = sns.color_palette("colorblind", len(self.p2)),
                              ax = ax)
            handles,labels = g.get_legend_handles_labels()
            plt.legend([], [], frameon = False)
            
            ax.set_xlabel(xlabel)
            ax.set_ylabel(None)
        
            #give left most plot y label.
            if legend:
                ax.set_ylabel(ylabel)
                legend = False

            #handles, labels = ax.get_legend_handles_labels()
            #new_labels = self.p2
            #if legend:
                #box = g.get_position()
                #g.set_position([box.x0, box.y0, box.width * 0.85, box.height]) # resize position
                #g.legend = None
                #legend = False
                #pylab.figlegend(*ax.get_legend_handles_labels(), loc = 'upper left')
            f.title = f"Population {i}" 
            f.tight_layout()
    
            # produce a legend for the objects in the other figure
            # save the two figures to files
            f.savefig(self.destination + title + self.param_keys[0] +  f"_{i}_" + "_gates_convergance.pdf")

        fig_legend = plt.figure(figsize = (3.5, 2.5))
        axi = fig_legend.add_subplot(111)   
        labels[0] = "Jump Window (R)"         
        fig_legend.legend(handles, labels, loc='center', title = None)
        axi.xaxis.set_visible(False)
        axi.yaxis.set_visible(False)
        axi.axis("off")
        fig_legend.canvas.draw()
        fig_legend.tight_layout()
        fig_legend.savefig(self.destination + "catplot_legend")

def main(experiment_function, source, destination, recall):
    experiment_function(source, destination, recall)

# %%
if __name__ == "__main__":

    print("Please use the ex@_depickle files for actual results. This is just the base class.")

