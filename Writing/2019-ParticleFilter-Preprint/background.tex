% !TEX root = ParticleFilter.tex
\section{Background\label{background}}

\subsection{Agent-Based Crowd Modelling}

One of the core strengths of agent-based modelling is its ability to replicate hetereogeneous individuals and their behaviours.  These individuals can be readily placed within an accurate representation of their physical environment, thus creating a potentially powerful tool for managing complex environments such as urban spaces. Understanding how pedestrians are likely to move around different urban environments is an important policy area.  Recent work using agent-based models have shown value in simulating applications from the design of railway stations \citep{klugl_largescale_2007, chen_multiagentbased_2017}, the organisations of queues \citep{kim_modeling_2013}, the dangers of crowding~\citep{helbing_simulating_2000} and the management of emergency evacuations \citep{vanderwal_simulating_2017}.

%Other examples could draw on:
%\begin{itemize}
%	\item PEDFLOW model (ABM of pedestrian behaviour) \url{https://www.researchgate.net/publication/245908302_DEVELOPING_THE_BEHAVIOURAL_RULES_FOR_AN_AGENT-BASED_MODEL_OF_PEDESTRIAN_MOVEMENT}
%	\item -Santos, G., \& Aguirre, B. E. (2004). A critical review of emergency evacuation simulation models.. 
%	-Templeton, A., Drury, J., \& Philippides, A. (2015). From mindless masses to small groups: Conceptualizing collective behavior in crowd modeling. Review of General Psychology, 19(3), 215.
%	Good review paper on evacuation or more general crowd models, showing the need for modelling social factors! (Not my work)
%	
%	\item Bosse, T., Hoogendoorn, M., Klein, M. C., Treur, J., Van Der Wal, C. N., \& Van Wissen, A. (2013). Modelling collective decision making in groups and crowds: Integrating social contagion and interacting emotions, beliefs and intentions. Autonomous Agents and Multi-Agent Systems, 27(1), 52-84.
%	This is one of my research again. Here we validated our crowd decision model including emotions and spread of emotions, intentions and beliefs, against other crowd models in a specific setting: the Dam square incident, 04-05-2010. Well cited article. 
%	
%	\item Zheng, X., Zhong, T., \& Liu, M. (2009). Modeling crowd evacuation of a building based on seven methodological approaches. Building and Environment, 44(3), 437-445.
%	A good review paper about seven approaches to model crowds evacuating. ABM is one of the approaches (vs fluid dynamics/gas or lattice models, animal experiments, cellular automate, etc). (Not my work). 
%\end{itemize}


However, a drawback with all agent-based crowd simulations -- aside from a handful of exceptions discussed below --  is that they are essentially models of the past. Historical data are used to estimate suitable model parameters and models are evolved forward in time regardless of any new data that might arise. Whilst this approach has value for exploring the dynamics of crowd behaviour, or for general policy or infrastructure planning, it means that models cannot be used to simulate crowd dynamics in \textit{real time}. The drivers of these systems are complex, hence the models are necessarily probabilistic.  This means that a collection of models (or an ensemble) will rapidly diverge from each other and from the real world even under identical starting conditions. This issue has fostered an emerging interest in the means of better associating models with empirical data from the target system \citep[see, e.g.,][]{wang_data_2015, ward_dynamic_2016, long_spatial_2017}. 


\subsection{Data-Driven Agent-Based Modelling}

The concept of Data-Driven Agent-Based Modelling (DDABM) emerged from broader work in data-driven application systems~\citep{darema_dynamic_2004} that aims to enhance or refine a model at runtime using new data. One of the most well developed models in this domain is the `WIPER' system~\citep{schoenharl_design_2011}.  This uses streaming data from mobile telephone towers to detect crisis events and model pedestrian behaviour.  When an event is detected, an ensemble of agent-based models are instantiated from streaming data and then validated in order to estimate which ensemble model most closely captured the particular crisis scenario \citep{schoenharl_evaluation_2008}. Although innovative in its use of streaming data, the approach is otherwise consistent with traditional methods for model validation based on historical data \citep{oloo_adaptive_2017a}. Similar attempts have been made to model solar panel adoption~\citep{zhang_datadriven_2015}, rail travel~\citep{othman_datadriven_2015}, crime \citep{lloyd_exploring_2016}, bird flocking \citep{oloo_predicting_2018} and aggregate pedestrian behaviour \citep{ward_dynamic_2016}, but whilst promising, these models contain their own limitations. For example, they either assume that agent behaviours can be proxied by simple regression models \citep{zhang_datadriven_2015} (which will make it impossible to use more advanced behavioural frameworks to encapsulate the more interesting features of agent behaviour), are calibrated manually \citep{othman_datadriven_2015} (which is infeasible in most cases), optimse model parameters dynamically but not the underlying model state (which might have diverged substantially from reality) \citep{oloo_predicting_2018}, or are simple enough to be approximated by an aggregate mathematical model \citep{lloyd_exploring_2016, ward_dynamic_2016}, neglecting the importance of using agent-based modelling in the first place.

There are two studies of direct relevance. The first is that of \citep{lueck_who_2019}, who developed an agent-based model of an evacuation coupled with a particle filter to conduct real-time data assimilation. This paper presents a novel mathematical approach that can map observations from a simple to a complex domain. In the authors' example, simulated data from hypothetical population counters are mapped to the complex agent-based model, which represents the heterogeneous locations of the individual agents. Although it is beyond the scope of the study here, the proposed mapping method will be useful for future work that experiments with variations in the complexity of the observations that are drawn from the real world (e.g. examining how well the particle filter can perform when presented with aggregate population counts rather than individual agent locations).

The second study is that of \citep{wang_data_2015} who investigated the viability of simulating individual movements in an indoor environment using streams of real-time sensor data to perform dynamic state estimation. As with this paper, the authors used an `identical twin' experimental framework; the agent-based model is used to create pseudo-realistic data rather than using those from real-world sensors. Next, an ensemble of models were developed to represent the target system with a particle filter used to constrain the models to the hypothetical reality. A new particle resampling method (`component set resampling') was also proposed that is shown to mitigate the particle deprivation problem (see Section~\ref{da_pf} for more details). The research presented within this paper builds on \citep{wang_data_2015} by: (i) attempting to apply data assimilation to a system that exhibits emergence; and (ii) performing more rigorous experiments to assess the conditions under which a particle filter is appropriate for assimilating data into agent-based crowd models.

\subsection{Data Assimilation and the Particle Filter\label{da_pf}}

 `Data assimilation' refers to a suite of mathematical approaches that are capable of using up-to-date data to adjust the state of a running model, allowing it to more accurately represent the current state of the target system. They have been successfully used in fields such as meteorology, where in recent years 7-day weather forecasts have become more accurate than 5-day forecasts were in the 1990s~\citep{bauer_quiet_2015}; partly due to improvements in data assimilation techniques~\citep{kalnay_atmospheric_2003}. The need for data assimilation was initially born out of data scarcity; numerical weather prediction models typically have two orders of magnitude more degrees of freedom than they do observation data. Initially the problem was addressed using interpolation~\citep[e.g.][]{panofsky_objective_1949, charney_dynamic_1951}, but this soon proved insufficient~\citep{kalnay_atmospheric_2003}. The eventual solution was to use a combination of \textit{observational data} and the \textit{predictions of short-range forecasts} (i.e. a model) to create the full initial conditions (the `\textit{first guess}') for a model of the current system state that could then  make forecasts. In effect, the basic premise is that by combining a detailed but uncertain model of the system with sparse but less uncertain data, ``all the available information'' can be used to estimate the true state of the target system~\citep{talagrand_use_1991}. 

A particle filter is only one of many different methods that have been developed to perform data assimilation. Others include the Successive Corrections Method, Optimal Interpolation, 3D-Var, 4D-Var, and various variants of Kalman Filtering \citep{carrassi_data_2018}, but it is beyond the scope of the paper to review all of these methods in detail. The particle filter method is chosen here because they are non-parametric methods and are better suited to performing data assimilation in systems that have non-linear and non-Gaussian behaviour~\citep{long_spatial_2017}, such as agent-based models. In fact, agent-based models are typically formulated as computer programs rather than in the mathematical forms required of many data assimilation methods, such as the Kalman filter and its variants \citep{wang_data_2015}.

% As  \citep[][p 37]{wang_data_2015} argue:\begin{quote}``A unique feature of the agent-based simulation model is that the model is specified by behaviours or rules and lacks the analytic structures (e.g., those in partial differential equation models) from which functional forms of probability distributions can be derived. This makes it difficult to apply conventional state estimation techniques such as Kalman filter and its variances.''\end{quote}

The particle filter is a brute force Bayesian state estimation method. The goal is to estimate a posterior distribution (i.e. the probability that the system is in a particular state conditioned on the observed data) using an ensemble of model instances, called particles. Each particle has an associated weight, normalised to sum to one, that are drawn from a prior distribution. In the data assimilation step (discussed shortly) each particle is confronted with observations from the (pseudo) real system and weights are adjusted depending on the likelihood that a particle could have produced the observations. Unlike most other data assimilation methods, a particle filter does not actually update the internal states of its particles. Instead, the worst performing particles -- those that are least likely to have generated the observations -- are removed from subsequent iterations, whereas better performing particles are duplicated. This has the advantage that, when performing data assimilation on an agent-based model, it is not necessary to derive a means of updating unstructured variables. For example, it is not clear how data assimilation approaches that have been designed for mathematical models that consist of purely numerical values will update models that contain agents with categorical  variables.

A particle filter (PF) $P_t$ at time $t$ with $N$ particles is the set 
\begin{equation}
P_t = \left\{ (x_t^i, w_t^i) : i \in \{1,\dots,N\} \right\},
\end{equation}
where $x_t^i$ is the state of the $i$-th particle with associated weight $w_t^i$, which are subject to the condition $\sum_{i=1}^N w_t^i = 1$ for all $t$. The general method of the particle filter is to use $P_t$ and new information in the form of observations to determine $P_{t+1}$. 

There are many different PF methods. The standard form is the sequential importance sampling (SIS) PF which selects the \emph{weights} using importance sampling \citep{bergman_recursive_1999, doucet_sequential_2000}. The particles are sampled from an importance density \citep{uosaki_nonlinear_2003}. One pitfall of the SIS PF is particle degeneracy. This occurs when the weights of all the particles tend to zero except for one particle which has a weight very close to one. This results in the population of particles being a very poor estimate for the posterior distribution.  One method to prevent particle degeneracy is to resample the \emph{particles}, duplicating particles with a high weight and discarding particles with a low weight. The probability of a particle being resampled is proportional to its weight; known as the sequential importance resampling (SIR) PF. The particles can be resampled in a variety of different ways, including multinomial, stratified, residual, systematic, and component set resampling \citep{liu_sequential_1998, douc_comparison_2005, wang_data_2015}. Although resampling helps to increase the spread of particle weights, it is often not sufficient \citep{carrassi_data_2018}. In a similar manner to particle degeneration in the SIS PF, \textit{particle collapse} can occur in the SIR PF. This occurs when only one particle is resampled so every particle has the same state. One of the main drawbacks with PF methods, as many studies have found \citep[e.g.][]{snyder_obstacles_2008, carrassi_data_2018}, is that the number of particles required to prevent particle degeneracy or collapse grows exponentially with the dimensionality of the model. This is an ongoing problem that will be revisited throughout the paper. It is worth nothing that there are many other PFs including the auxiliary SIR PF and the regularised PF \citep{arulampalam_tutorial_2002}, the merging PF \citep{nakano_merging_2007}, and the resample-move PF \citep{gilks_following_2001}. In Section ~\ref{particle_filter}, we will consider a SIR PF with systematic resampling because it ranks higher in resampling quality and computational simplicity compared to other resampling methods \citep{hol_resampling_2006, douc_comparison_2005}.
