% !TEX root = ParticleFilter.tex
\section{Discussion and Future Work\label{discussion}}

\subsection{Discussion}

This paper has experimented with the use of a sequential importance resampling (SIR) particle filter (PF) as a means of dynamically incorporating data into a simple agent-based model of a pedestrian system. The results demonstrate that it is possible to use a particle filter to perform dynamic adjustment of the model. However, they also show that (as expected~\citep{rebeschini_can_2015, carrassi_data_2018}) as the dimensionality of the system increases, the number of particles required to maintain an acceptable approximation error grows exponentially. The reason for this is because, as the dimensionality increases, it becomes less likely that an individual particle will have the `correct combination' of values~\citep{long_spatial_2017}. In this work, the dimensionality is proportional to the number of agents. At most 10,000 particles were used, which was sufficient for a simulation with 30-40 agents. However, for a more complex and realistic model containing hundreds or thousands of agents, the number of particles required would most likely number in the millions. The particle filter used in this study was provided with more information than would normally be available. For example, information was supplied as fixed parameters on when each agent entered the simulation, their maximum speeds, and their chosen destinations. Therefore the only information that the particle filter was lacking was the actual locations of the agents and whether they would chose to move left or right to prevents a collision with another agent. It is entirely possible to include these agent-level parameters in the state vector, but this would further increase the size of the state space and hence the number of particles required. This is an important caveat as in a real-world situation it is very unlikely that such detail would be available. Future work should begin to experiment with the number of particles that would be required when observational data that are more akin to those available in the real world are used.

There are a number of possible improvements that could be made to the basic SIR particle filter to reduce the number of particles required. For example, \citep{wang_data_2015} propose \textit{component set resampling} -- details below -- but exploring these further is beyond the scope of this paper. Overall, these results reflect those of other studies which demonstrate that particle filtering has value for simulations with relatively few agents and interactions \citep[e.g.][]{wang_data_2015, lueck_who_2019, kieu_dealing_2019} but that the large dimensionality of a pedestrian system poses problems for the standard (unmodified) bootstrap filter. 

\subsection{Improvements to the particle filter}

There are a number of adaptions to the main particle filtering algorithm that might make the method more amenable to use with complex agent-based models. The aforementioned Component Set Resampling \citep{wang_data_2015} approach proposes that individual components of particles are sampled, rather than whole particles in their entirety. A more commonly used approach is to reduce the dimensionality of the problem in the first place. With spatial agent-based models, such as the one used here, spatial aggregation provides such an opportunity. In the data assimilation stage, the state vector could be converted to an aggregate form, such as a population density surface, and particle filtering could be conducted on the aggregate surface rather than on the individual locations of the agents. After assimilation, the particles could be disaggregated and then run as normal. This will, of course, introduce error because the exact positions of agents in the particles will not be known when disaggregating, but that additional uncertainty might be outweighed by the benefits of a more successful particle filtering overall. In addition, the observations that could be presented to an aggregate particle filter might be much more in line with those that are available in practice (as will be discussed shortly). If the aim of real-time model calibration is to give decision makers a \textit{general idea} about how the system is behaving in real time, then this additional uncertainty might not be problematic. A similar approach, proposed by \citep{rebeschini_can_2015}, could be to divide up the space into smaller regions and then apply the algorithm locally to these regions (i.e. a divide-and-conquer approach). Although it is not clear how well divide-and-conquer would work in an agent-based model -- for example, \citep{long_spatial_2017} developed the method for a discrete cellular automata model -- it would be an interesting avenue to explore.

\subsection{Real-World Implications and Future Work}

It is important to note that, unlike other data assimilation approaches, the particle filter does not dynamically alter the state of the running model. This could be advantageous because, with agent-based modelling, it is not clear that the state of an agent should be manipulated by an external process. Agents typically have goals and a history, and behavioural rules that rely on those features, so artificially altering an agent's internal state might disrupt their behaviour making it, at worst, nonsensical. Experiments with alternative (potentially more efficient) algorithms such as 4DVar or a the Ensemble Kalman Filter should be conducted to test this. 

Ultimately the aim of this work is to develop methods that will allow simulations of human pedestrian systems to be optimised in real time. Not only will such methods provide decision makers with more accurate information about the present, but they could also allow for better predictions of the near future \citep[c.f.][]{kieu_dealing_2019}. One assumption made throughout the paper, which limits its direct real-world use, is that the locations of pseudo-real individuals are known, albeit with some uncertainty. Not only is this assumption unrealistic -- it is rare for individuals to be tracked to this degree in public places -- but we would also argue that the privacy implications of tracking individual people are not outweighed by the benefits offered by better understanding the system. Therefore, immediate future work will test how well a data assimilation algorithm would fare were it supplied only aggregate information such as the number of people who pass through a barrier, or the number of people recorded by a CCTV camera within a particular area. Both of these measures can be used in aggregate form and would be entirely anonymous. It is unclear whether such aggregate data would be sufficient to identify a `correct' agent-based model, so experiments should explore the spatio-temporal resolution of the aggregate data are required. Also, identifiability/equifinality analysis might help initially as a means of estimating whether the available data are sufficient to identify a seemingly `correct' model in the first place. In the end, such research might help to provide evidence to policy makers for the number, and characteristics, of the sensors that would need to be installed to accurately simulate the target system, and how these could be used to maintain the privacy of the people who they are recording.



